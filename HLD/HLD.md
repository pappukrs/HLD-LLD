# HLD Building Blocks - Master System Design Fundamentals

## üéØ What is High Level Design (HLD)?

HLD focuses on the **overall system architecture** - defining major components, their interactions, technology choices, and how the system scales to handle millions of users.

**Mental Model:** Think of HLD as designing a city's infrastructure. You're planning the roads (network), power plants (servers), water systems (data flow), and how everything connects to serve millions of residents.

### HLD vs LLD

| Aspect | HLD | LLD |
|--------|-----|-----|
| **Scope** | System architecture | Class/module design |
| **Focus** | What components, how they interact | How each component works internally |
| **Scale** | Millions of users, distributed systems | Single service implementation |
| **Tools** | Databases, Load Balancers, Caches | Classes, Interfaces, Design Patterns |
| **Questions** | "How to scale?" | "How to implement?" |
| **Example** | Design Twitter's architecture | Implement Tweet class with OOP |

---

## üß± PART 1: Core System Design Principles

### 1Ô∏è‚É£ Scalability - The Foundation

**Scalability** = System's ability to handle growth (users, data, traffic)

#### Vertical Scaling (Scale Up)
**What:** Add more power to existing machine

```
Single Server Upgrade:
4 CPU  ‚Üí 16 CPU
8 GB RAM ‚Üí 64 GB RAM
100 GB SSD ‚Üí 1 TB SSD
```

**Pros:**
- ‚úÖ Simple - no code changes
- ‚úÖ No data consistency issues
- ‚úÖ Inter-process communication is fast

**Cons:**
- ‚ùå Hardware limits (can't scale infinitely)
- ‚ùå Single point of failure
- ‚ùå Expensive (non-linear cost)
- ‚ùå Downtime during upgrades

**When to Use:**
- Small to medium applications
- Traditional databases that need ACID
- Initial MVP stage

---

#### Horizontal Scaling (Scale Out)
**What:** Add more machines

```
Single Server ‚Üí Multiple Servers
1 x 16 CPU ‚Üí 10 x 4 CPU
1 x 64GB RAM ‚Üí 10 x 8GB RAM
```

**Pros:**
- ‚úÖ Unlimited scaling (add more servers)
- ‚úÖ Better fault tolerance (redundancy)
- ‚úÖ Cost-effective (commodity hardware)
- ‚úÖ No downtime (add servers incrementally)

**Cons:**
- ‚ùå Complex architecture
- ‚ùå Data consistency challenges
- ‚ùå Load balancing required
- ‚ùå Network latency overhead

**When to Use:**
- Web applications with high traffic
- Microservices architecture
- Systems requiring high availability

---

#### Auto-Scaling

**Cloud-based dynamic scaling**

```yaml
Auto-Scaling Rules:
  Min Instances: 3
  Max Instances: 20
  
  Scale-Out Trigger:
    - CPU > 70% for 5 minutes ‚Üí Add 2 instances
    - Requests/sec > 1000 ‚Üí Add 3 instances
  
  Scale-In Trigger:
    - CPU < 30% for 10 minutes ‚Üí Remove 1 instance
    - Keep minimum 3 instances running
```

**Metrics to Monitor:**
- CPU utilization
- Memory usage
- Request queue length
- Response time
- Network I/O

---

### 2Ô∏è‚É£ CAP Theorem - The Fundamental Trade-off

**CAP Theorem:** In a distributed system, you can guarantee only **2 out of 3**:

```
        Consistency
           /  \
          /    \
         /      \
        / Choose \
       /    2     \
      /____________\
Partition         Availability
Tolerance
```

#### **C - Consistency**
All nodes see the same data at the same time

**Example:**
```
Bank Account Balance:
User A reads balance: $100
User B withdraws $50
User A reads again: Must see $50 (consistent)
```

#### **A - Availability**
System responds to every request (even during failures)

**Example:**
```
Social Media Post:
Instagram temporarily down in one region
‚Üí Still shows cached feed (available but maybe stale)
```

#### **P - Partition Tolerance**
System works despite network failures between nodes

**Example:**
```
Network Split:
Server 1 (US) ‚ÜêX‚Üí Server 2 (EU)
Both continue operating independently
```

---

#### CAP Combinations

| System Type | Choice | Examples | Use Case |
|-------------|--------|----------|----------|
| **CP** | Consistency + Partition | MongoDB, HBase, Redis (certain modes) | Banking, Financial systems |
| **AP** | Availability + Partition | Cassandra, DynamoDB, Couchbase | Social media, Analytics |
| **CA** | Consistency + Availability | PostgreSQL, MySQL (single node) | Traditional RDBMS |

**Reality Check:** CA systems are rare in distributed environments because network partitions WILL happen.

---

#### Practical Examples

**CP System - Banking:**
```
Scenario: Transfer $100 from Account A to B

Option 1 (CP): Wait for all replicas to confirm
  ‚Üí Might be slow, but balance is always correct
  
Option 2 (AP): Respond immediately, sync later
  ‚Üí Fast, but risk of double-spending
  
Banks choose CP ‚Üí Consistency over speed
```

**AP System - Twitter:**
```
Scenario: User posts a tweet

Option 1 (CP): Wait for all followers' timelines to update
  ‚Üí Slow for celebrities with millions of followers
  
Option 2 (AP): Accept post, propagate asynchronously
  ‚Üí Fast, followers see tweet eventually
  
Twitter chooses AP ‚Üí Availability over immediate consistency
```

---

### 3Ô∏è‚É£ Latency vs Throughput

#### Latency
**Time to complete a single request**

```
Web Request Lifecycle:
DNS Lookup:        20-120 ms
TCP Connection:    20-50 ms
TLS Handshake:     100-200 ms
HTTP Request:      50-200 ms
Server Processing: 50-500 ms
HTTP Response:     50-200 ms
Total Latency:     ~500-1000 ms (1 second)
```

**Latency Numbers Everyone Should Know:**
```
L1 cache reference:              0.5 ns
Branch mispredict:               5 ns
L2 cache reference:              7 ns
Mutex lock/unlock:               25 ns
Main memory reference:           100 ns
Send 2K bytes over network:      20,000 ns (20 ¬µs)
Read 1 MB from SSD:              1,000,000 ns (1 ms)
Disk seek:                       10,000,000 ns (10 ms)
Read 1 MB from disk:             20,000,000 ns (20 ms)
Send packet US ‚Üí EU ‚Üí US:        150,000,000 ns (150 ms)
```

**Reducing Latency:**
- Use CDN (reduce network distance)
- Add caching (avoid repeated work)
- Database indexing (faster queries)
- Async processing (don't wait)
- Connection pooling (reuse connections)

---

#### Throughput
**Requests processed per unit time**

```
Web Server Capacity:
Single Server: 1,000 req/s
  
Horizontal Scaling:
10 Servers: 10,000 req/s
100 Servers: 100,000 req/s

Database Throughput:
MySQL: ~10,000 writes/s
Cassandra: ~1,000,000 writes/s
```

**Increasing Throughput:**
- Horizontal scaling (more servers)
- Load balancing (distribute work)
- Batch processing (group operations)
- Async processing (non-blocking)
- Caching (reduce database load)

---

#### The Trade-off

```
Low Latency vs High Throughput:

Scenario 1: Single request with all optimizations
  ‚Üí Latency: 10ms
  ‚Üí Throughput: 100 req/s (sequential)

Scenario 2: Batch 100 requests together
  ‚Üí Latency: 1000ms (for all 100)
  ‚Üí Throughput: 100,000 req/s

Trade-off: Higher throughput often means higher latency
```

**Real Example - Database Writes:**
```
Write Strategy 1: Immediate write
  - Latency: 10ms per write
  - Throughput: 100 writes/s
  
Write Strategy 2: Batch writes every 100ms
  - Latency: Up to 100ms per write
  - Throughput: 10,000 writes/s
  
Choose based on requirements:
  - Banking ‚Üí Low latency (immediate confirmation)
  - Analytics ‚Üí High throughput (batch is fine)
```

---

## üîß PART 2: Essential Building Blocks

### 1Ô∏è‚É£ Load Balancer - Traffic Distribution

**Purpose:** Distribute incoming requests across multiple servers

```
         Users (millions)
              ‚Üì
      [Load Balancer]
         /    |    \
      [S1]  [S2]  [S3]
      App   App   App
     Servers
```

#### Types of Load Balancers

**Layer 4 (Transport Layer) - L4 LB**
```
Operates on: IP Address + Port
Fast: No packet inspection
Use Case: High throughput required

Example:
Client IP: 192.168.1.5:5000
  ‚Üí Routes to: Server 10.0.1.10:80
```

**Layer 7 (Application Layer) - L7 LB**
```
Operates on: HTTP Headers, URLs, Cookies
Smart: Can read request content
Use Case: Content-based routing

Example:
Request: GET /api/users ‚Üí API Server Pool
Request: GET /images/logo.png ‚Üí Static Server Pool
Request: Cookie(user=premium) ‚Üí Premium Server Pool
```

---

#### Load Balancing Algorithms

**1. Round Robin**
```
Request 1 ‚Üí Server 1
Request 2 ‚Üí Server 2
Request 3 ‚Üí Server 3
Request 4 ‚Üí Server 1 (repeat)

Pros: Simple, fair distribution
Cons: Ignores server load
Use When: All servers have equal capacity
```

**2. Least Connections**
```
Server 1: 10 active connections
Server 2: 5 active connections  ‚Üê Send here
Server 3: 8 active connections

Pros: Better for long-lived connections
Cons: More complex tracking
Use When: Requests have varying duration
```

**3. Weighted Round Robin**
```
Server 1 (8 CPU): Weight = 8
Server 2 (4 CPU): Weight = 4
Server 3 (2 CPU): Weight = 2

Distribution: 8:4:2 ratio
S1 gets 57%, S2 gets 29%, S3 gets 14%

Use When: Servers have different capacities
```

**4. IP Hash**
```
hash(client_IP) % num_servers = assigned_server

Client 192.168.1.5 ‚Üí hash ‚Üí Server 2
Always routes to same server

Pros: Session affinity (sticky sessions)
Cons: Uneven distribution possible
Use When: Need consistent routing per user
```

**5. Least Response Time**
```
Server 1: Avg response = 200ms
Server 2: Avg response = 50ms  ‚Üê Send here
Server 3: Avg response = 150ms

Pros: Best user experience
Cons: Requires health monitoring
Use When: Latency is critical
```

---

#### Health Checks

```yaml
Health Check Configuration:
  Interval: 10 seconds
  Timeout: 5 seconds
  Healthy Threshold: 2 consecutive successes
  Unhealthy Threshold: 3 consecutive failures
  
  Check Types:
    - TCP: Can connect to port 80?
    - HTTP: GET /health returns 200 OK?
    - Custom: Application-specific logic
```

**Example Flow:**
```
Normal State:
LB ‚Üí Server 1 (healthy) ‚Üê Receives traffic
LB ‚Üí Server 2 (healthy) ‚Üê Receives traffic

Server 2 fails health check:
LB ‚Üí Server 1 (healthy) ‚Üê Gets all traffic
LB ‚Üí Server 2 (unhealthy) ‚Üê No traffic, quarantined

Server 2 recovers:
After 2 successful checks ‚Üí Back in rotation
```

---

#### Popular Load Balancers

| Tool | Type | Best For |
|------|------|----------|
| **Nginx** | L7 | Web applications, reverse proxy |
| **HAProxy** | L4/L7 | High performance, TCP/HTTP |
| **AWS ELB** | L4/L7 | AWS cloud, auto-scaling |
| **Google Cloud LB** | L4/L7 | GCP cloud, global distribution |
| **F5** | L4/L7 | Enterprise, hardware appliance |

---

### 2Ô∏è‚É£ Caching - Speed Through Memory

**Purpose:** Store frequently accessed data in fast storage

```
Request Flow Without Cache:
Client ‚Üí Server ‚Üí Database (slow, 50-100ms)
                     ‚Üë
                  Every request hits DB

Request Flow With Cache:
Client ‚Üí Server ‚Üí Cache (fast, 1-5ms) ‚Üí Found? Return
                     ‚Üì
                  Not found? ‚Üí Database ‚Üí Store in cache
```

---

#### Cache Levels

**1. Client-Side Cache (Browser)**
```html
<!-- HTTP Headers -->
Cache-Control: max-age=3600 (cache for 1 hour)
ETag: "abc123" (version identifier)
Expires: Wed, 21 Oct 2025 07:28:00 GMT

Use Case: Static assets (images, CSS, JS)
Pro: Zero server load
Con: Can't invalidate easily
```

**2. CDN (Content Delivery Network)**
```
User in Tokyo ‚Üí Tokyo CDN Edge ‚Üí Serve cached content
User in London ‚Üí London CDN Edge ‚Üí Serve cached content
                      ‚Üì
                  Only miss hits origin server

Use Case: Global static content
Pro: Low latency worldwide
Con: Cost, cache invalidation complexity
```

**3. Application Cache (Redis/Memcached)**
```
Application Server:
  1. Check Redis
  2. If hit ‚Üí Return (1ms)
  3. If miss ‚Üí Query DB (50ms)
                ‚Üí Store in Redis
                ‚Üí Return

Use Case: Session data, API responses, computed results
Pro: Flexible, programmable
Con: Need to manage consistency
```

**4. Database Cache**
```
Database Query Cache:
SELECT * FROM users WHERE id = 123
  ‚Üí

 Cached in DB memory
  ‚Üí Next same query returns instantly

Use Case: Repeated queries
Pro: Automatic
Con: Limited control, invalidation can be tricky
```

---

### 3Ô∏è‚É£ CQRS (Command Query Responsibility Segregation)

**What:** Separate read and write operations into different models

```
Traditional:
[Single Database] ‚Üê Both reads and writes

CQRS:
Write Model (Commands):
  CreateOrder, UpdateUser, DeleteProduct
  ‚Üí [Write DB] (optimized for writes)
       ‚Üì (sync)
  [Read DB] (optimized for reads, denormalized)
       ‚Üë
Read Model (Queries):
  GetUserProfile, ListOrders, SearchProducts
```

**Benefits:**
- ‚úÖ Optimize each side independently
- ‚úÖ Scale reads and writes separately
- ‚úÖ Simpler queries (read model can be denormalized)
- ‚úÖ Better performance

**When to Use:**
- Complex domain with different read/write patterns
- High read:write ratio (different scaling needs)
- Event sourcing systems

---

### 4Ô∏è‚É£ Saga Pattern - Distributed Transactions

**Problem:** How to maintain data consistency across multiple services?

```
Traditional Transaction (Single DB):
BEGIN TRANSACTION
  Deduct from Account A
  Add to Account B
COMMIT
‚Üí Either both happen or neither (ACID)

Microservices (Multiple DBs):
  Order Service DB
  Payment Service DB
  Inventory Service DB
‚Üí Can't use traditional transactions!
```

**Solution: Saga Pattern**

**Choreography-Based Saga:**
```
1. Order Service: Create order ‚Üí Emit "OrderCreated" event
2. Payment Service: Charge card ‚Üí Emit "PaymentCompleted" event
3. Inventory Service: Reserve items ‚Üí Emit "InventoryReserved" event
4. Order Service: Mark order complete

If any step fails:
  ‚Üí Emit compensating events
  PaymentFailed ‚Üí RefundPayment ‚Üí CancelOrder
```

**Orchestration-Based Saga:**
```
Saga Orchestrator controls flow:
1. Call Order Service
2. If success ‚Üí Call Payment Service
3. If success ‚Üí Call Inventory Service
4. If any fails ‚Üí Execute compensating transactions in reverse
```

**Trade-offs:**
- Choreography: Simpler, loosely coupled, harder to track
- Orchestration: Centralized control, easier to debug, single point of failure

---

### 5Ô∏è‚É£ Circuit Breaker Pattern

**Purpose:** Prevent cascading failures

```
Circuit States:

CLOSED (Normal):
  Service A ‚Üí Service B (working fine)

OPEN (Failing):
  Service B is down
  ‚Üí Stop calling Service B immediately
  ‚Üí Return cached/default response
  ‚Üí Save resources, fail fast

HALF-OPEN (Testing):
  After cooldown period
  ‚Üí Try a few test requests
  ‚Üí If success ‚Üí CLOSED
  ‚Üí If fail ‚Üí OPEN again
```

**Implementation:**
```python
class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_count = 0
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.state = "CLOSED"
        self.last_failure_time = None
    
    def call(self, func):
        if self.state == "OPEN":
            if time.now() - self.last_failure_time > self.timeout:
                self.state = "HALF-OPEN"
            else:
                raise CircuitBreakerOpen("Service unavailable")
        
        try:
            result = func()
            self.on_success()
            return result
        except Exception as e:
            self.on_failure()
            raise e
    
    def on_success(self):
        self.failure_count = 0
        self.state = "CLOSED"
    
    def on_failure(self):
        self.failure_count += 1
        self.last_failure_time = time.now()
        if self.failure_count >= self.failure_threshold:
            self.state = "OPEN"
```

---

## üéØ PART 4: The 5-Step System Design Interview Framework

### Overview (45-minute interview)

```
Step 1: Requirements Clarification        (5-7 min)
Step 2: Capacity Estimation              (3-5 min)
Step 3: High-Level Design                (10-15 min)
Step 4: Deep Dive                        (15-20 min)
Step 5: Discussion & Trade-offs          (5 min)
```

---

### Step 1: Requirements Clarification (5-7 min)

**Template Questions:**

**Functional Requirements:**
```
‚úì What are the core features?
‚úì Who are the users?
‚úì What are the main use cases?
‚úì Mobile, web, or both?
‚úì What's out of scope?
```

**Non-Functional Requirements:**
```
‚úì Scale: How many users (DAU/MAU)?
‚úì Performance: What's acceptable latency?
‚úì Availability: Can we have downtime?
‚úì Consistency: Strong or eventual?
‚úì Geography: Single region or global?
```

**Example: Design Twitter**
```
Interviewer: "Design Twitter"

You: "Let me clarify the requirements:

Functional:
1. Users can post tweets (280 chars)?
2. Users can follow other users?
3. Users see home timeline (tweets from followed users)?
4. Search tweets?
5. Trending topics?
6. Direct messaging? (Maybe out of scope)

Non-Functional:
1. How many users? (Assume 500M DAU)
2. Read:Write ratio? (Assume 100:1, read-heavy)
3. Latency for timeline? (< 500ms acceptable)
4. Eventual consistency okay for timeline?
5. Global or single region? (Global)

Out of Scope:
- Video uploads
- Ads
- Analytics dashboard
```

---

### Step 2: Capacity Estimation (3-5 min)

**Estimation Framework:**

```python
# Given: 500M DAU (Daily Active Users)

# 1. QPS (Queries Per Second)
Assumptions:
- Each user posts 2 tweets/day
- Each user views timeline 20 times/day

Write QPS:
  500M users √ó 2 tweets/day = 1B tweets/day
  1B / 86,400 seconds ‚âà 12,000 tweets/second
  Peak (3x) = 36,000 writes/second

Read QPS:
  500M users √ó 20 views/day = 10B timeline views/day
  10B / 86,400 ‚âà 115,000 reads/second
  Peak = 345,000 reads/second

# 2. Storage
Tweet size:
  tweet_id: 8 bytes
  user_id: 8 bytes
  text: 280 bytes
  timestamp: 8 bytes
  metadata: 100 bytes
  Total ‚âà 400 bytes/tweet

Daily storage:
  1B tweets/day √ó 400 bytes = 400 GB/day
  Annual: 400 GB √ó 365 ‚âà 146 TB/year
  5 years: 730 TB
  With replication (3x): ~2.2 PB

# 3. Bandwidth
Read bandwidth:
  115K req/s √ó 400 bytes √ó 20 tweets/timeline
  = 115K √ó 8KB ‚âà 920 MB/second (read)

Write bandwidth:
  12K req/s √ó 400 bytes ‚âà 4.8 MB/second (write)
```

**Cheat Sheet:**
```
Powers of 2:
2^10 = 1 KB = 1,000 bytes
2^20 = 1 MB = 1,000,000 bytes
2^30 = 1 GB = 1,000,000,000 bytes

Time:
1 day = 86,400 seconds ‚âà 100K seconds
1 month ‚âà 2.5M seconds
1 year ‚âà 30M seconds

Rule of thumb:
1M users ‚Üí ~10 QPS (if 20% DAU, each makes 5 requests/day)
```

---

### Step 3: High-Level Design (10-15 min)

**Draw the architecture:**

```
1. Start with client and servers
2. Add load balancer
3. Add application servers
4. Add databases (with reasoning)
5. Add cache
6. Add CDN (if static content)
7. Add message queues (if async needed)

Example Twitter:
[Mobile/Web Client]
        ‚Üì
  [Load Balancer]
        ‚Üì
  [API Gateway]
    /    |    [Tweet] [Timeline] [User]
Service  Service   Service
  ‚Üì        ‚Üì         ‚Üì
[Tweet] [Timeline] [User]
  DB      Cache      DB
  ‚Üì
[Kafka] ‚Üí Fanout Service ‚Üí Timeline Cache
```

**API Design:**
```
POST /api/v1/tweets
  Request: {user_id, text, media_urls[]}
  Response: {tweet_id, timestamp}

GET /api/v1/timeline
  Request: {user_id, page_token}
  Response: {tweets[], next_page_token}

POST /api/v1/follow
  Request: {follower_id, followee_id}
  Response: {success}
```

**Database Schema:**
```sql
-- Users table
CREATE TABLE users (
    user_id BIGINT PRIMARY KEY,
    username VARCHAR(50) UNIQUE,
    email VARCHAR(100),
    created_at TIMESTAMP
);

-- Tweets table (Cassandra-like, time-series)
CREATE TABLE tweets (
    tweet_id BIGINT PRIMARY KEY,
    user_id BIGINT,
    text VARCHAR(280),
    created_at TIMESTAMP,
    likes_count INT,
    retweets_count INT
);

-- Followers (graph data, maybe Neo4j)
CREATE TABLE followers (
    follower_id BIGINT,
    followee_id BIGINT,
    created_at TIMESTAMP,
    PRIMARY KEY (follower_id, followee_id)
);

-- Timeline cache (Redis)
Key: "timeline:{user_id}"
Value: [tweet_id1, tweet_id2, ..., tweet_id100]
```

---

### Step 4: Deep Dive (15-20 min)

**Discuss interviewer's focus areas**

**Common Deep Dives:**

**A. Timeline Generation (Fan-out problem)**

```
Problem: User with 1M followers posts tweet
  ‚Üí Do we update 1M timelines immediately?

Approach 1: Fan-out on Write
  When tweet posted:
    1. Get all follower IDs
    2. For each follower:
         Insert tweet_id into their timeline cache
  
  Pros: Fast reads (pre-computed)
  Cons: Slow writes (celebrity problem)

Approach 2: Fan-out on Read
  When user requests timeline:
    1. Get all users they follow
    2. Get recent tweets from each
    3. Merge and sort
  
  Pros: Fast writes
  Cons: Slow reads (must query multiple users)

Hybrid Approach (Twitter's actual solution):
  Normal users (<1000 followers): Fan-out on write
  Celebrities (>1M followers): Fan-out on read
  Medium users (1K-1M): Hybrid
```

**B. Scaling Databases**

```
Sharding Strategy (User-based):
  Shard = hash(user_id) % num_shards
  
  Pros: Even distribution
  Cons: Cross-shard queries hard (e.g., get all tweets from followed users)

Sharding Strategy (Tweet-based):
  Shard by timestamp (time-series)
  Recent tweets ‚Üí Hot shard
  Old tweets ‚Üí Cold storage
  
  Pros: Good for time-based queries
  Cons: Hot shard bottleneck
```

**C. Caching Strategy**

```
Cache Layers:
1. CDN: Profile images, static assets
2. Redis: Timelines (per user), trending topics
3. Application cache: User session data

Eviction:
  Timeline cache: Keep last 100 tweets per user
  Trending cache: TTL 5 minutes (refresh frequently)
```

---

### Step 5: Discussion & Trade-offs (5 min)

**Topics to Cover:**

**Monitoring:**
```
Metrics:
- QPS per service
- Latency (p50, p95, p99)
- Error rates
- Cache hit rate

Alerts:
- Latency > 1 second
- Error rate > 1%
- Database connection pool exhausted
```

**Security:**
```
- Authentication: JWT tokens
- Authorization: Check tweet ownership before delete
- Rate limiting: Prevent spam (100 tweets/hour)
- Input validation: Sanitize text, prevent XSS
```

**Future Improvements:**
```
- ML-based timeline ranking
- Real-time notifications (WebSocket)
- Video uploads
- Distributed tracing
- Multi-region deployment
```

---

## üìö PART 5: Complete Problem Solutions (30 Questions)

### Easy/Foundational (5)

#### 1. Design URL Shortener (TinyURL)

**Requirements:**
- Shorten long URLs
- Redirect to original
- Analytics (click count)
- Custom URLs
- Expiry time

**Capacity:**
- 100M new URLs/month
- Read:Write = 100:1
- 10B redirects/month

**High-Level Design:**
```
[Client]
   ‚Üì
[Load Balancer]
   ‚Üì
[API Servers]
   ‚Üì         ‚Üì
[Cache]   [Database]
(Redis)   (NoSQL)
```

**Key Design Decisions:**

**1. Short URL Generation:**
```python
# Base62 encoding (a-z, A-Z, 0-9)
def encode_base62(num):
    chars = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
    result = ""
    while num > 0:
        result = chars[num % 62] + result
        num //= 62
    return result

# Example:
# ID 12345 ‚Üí encode_base62(12345) ‚Üí "3D7"
# URL: tinyurl.com/3D7

# With 6 characters:
# 62^6 = 56 billion possible URLs
```

**2. Database Schema:**
```sql
-- URLs table
CREATE TABLE urls (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    short_code VARCHAR(10) UNIQUE,
    original_url TEXT,
    user_id BIGINT,
    created_at TIMESTAMP,
    expires_at TIMESTAMP,
    click_count INT DEFAULT 0
);

-- Index for fast lookups
CREATE INDEX idx_short_code ON urls(short_code);
```

**3. API Design:**
```
POST /api/v1/shorten
  Request: {
    "long_url": "https://example.com/very/long/url",
    "custom_code": "mylink" (optional),
    "expiry_hours": 24 (optional)
  }
  Response: {
    "short_url": "https://tinyurl.com/mylink",
    "expires_at": "2025-01-15T10:00:00Z"
  }

GET /{short_code}
  ‚Üí 302 Redirect to original URL
  ‚Üí Increment click count (async)
```

**4. Scaling:**
```
Caching:
  Cache popular URLs in Redis
  Key: "url:{short_code}"
  Value: {original_url, expires_at}
  TTL: 1 hour

Write Optimization:
  Click count updates ‚Üí Message queue
  Batch update database every minute
```

---

#### 2. Design Rate Limiter

**Requirements:**
-Limit requests per user/IP
- Different limits per API
- Distributed system
- Low latency overhead

**Algorithms:**

**Token Bucket (Recommended):**
```python
class TokenBucket:
    def __init__(self, capacity, refill_rate):
        self.capacity = capacity
        self.tokens = capacity
        self.refill_rate = refill_rate  # tokens per second
        self.last_refill = time.now()
    
    def allow_request(self):
        self.refill()
        
        if self.tokens >= 1:
            self.tokens -= 1
            return True
        return False
    
    def refill(self):
        now = time.now()
        elapsed = now - self.last_refill
        tokens_to_add = elapsed * self.refill_rate
        
        self.tokens = min(self.capacity, self.tokens + tokens_to_add)
        self.last_refill = now

# Usage
bucket = TokenBucket(capacity=100, refill_rate=10)  # 100 tokens, refill 10/sec
if bucket.allow_request():
    process_request()
else:
    return "429 Too Many Requests"
```

**Redis Implementation:**
```python
def is_allowed(user_id, limit, window):
    key = f"rate_limit:{user_id}"
    current = redis.get(key)
    
    if current is None:
        redis.setex(key, window, 1)
        return True
    
    if int(current) < limit:
        redis.incr(key)
        return True
    
    return False

# Fixed window counter
# User can make 100 requests per minute
is_allowed("user123", limit=100, window=60)
```

**Architecture:**
```
[Client Request]
       ‚Üì
[API Gateway with Rate Limiter]
       ‚Üì Check Redis
[Redis Cluster]
  Key: "rate:{user_id}:{api}"
  Value: {tokens, last_refill_time}
       ‚Üì
[Backend Services] (if allowed)
```

---

### Medium Problems (15)

#### 6. Design YouTube/Netflix

**Requirements:**
- Upload videos
- Stream videos (adaptive bitrate)
- Recommendations
- Comments, likes
- Search

**Scale:**
- 1B users
- 500 hours uploaded/minute
- 1B video views/day

**Architecture:**
```
Upload Flow:
[User] ‚Üí [Upload Service] ‚Üí [Object Storage (S3)]
                          ‚Üì
                    [Transcoding Service]
                          ‚Üì
         Multiple formats (480p, 720p, 1080p, 4K)
                          ‚Üì
                        [CDN]

Streaming Flow:
[User] ‚Üí [CDN Edge] ‚Üí [Origin Server] ‚Üí [Object Storage]

Recommendation:
[User Activity] ‚Üí [Kafka] ‚Üí [ML Service] ‚Üí [Recommendation DB]
```

**Key Components:**

**1. Video Processing:**
```
Upload ‚Üí Split into chunks ‚Üí Parallel transcoding
  Input: 4K video
  Output:
    - 480p (mobile)
    - 720p (HD)
    - 1080p (Full HD)
    - 4K (Ultra HD)
  
  Each format:  HLS segments (10-second chunks)
```

**2. Adaptive Bitrate Streaming:**
```
Client measures bandwidth:
  High bandwidth ‚Üí 1080p
  Medium ‚Üí 720p
  Low ‚Üí 480p
  
Seamless switching between quality levels
```

**3. Database Design:**
```sql
-- Video metadata (PostgreSQL)
CREATE TABLE videos (
    video_id BIGINT PRIMARY KEY,
    title VARCHAR(200),
    description TEXT,
    uploader_id BIGINT,
    duration INT,
    view_count BIGINT,
    like_count INT,
    upload_date TIMESTAMP
);

-- Video files (metadata, actual files in S3)
CREATE TABLE video_files (
    file_id BIGINT PRIMARY KEY,
    video_id BIGINT,
    resolution VARCHAR(10),  -- 480p, 720p, etc.
    format VARCHAR(10),       -- mp4, webm
    s3_url TEXT,
    file_size BIGINT
);

-- Comments (Cassandra, high write)
CREATE TABLE comments (
    comment_id BIGINT,
    video_id BIGINT,
    user_id BIGINT,
    text TEXT,
    created_at TIMESTAMP,
    PRIMARY KEY (video_id, created_at, comment_id)
);
```

**4. CDN Strategy:**
```
Popular videos: Cached at all edge locations
Less popular: Cached on-demand
Very rare: Pulled from origin

Cache Eviction: LRU with view count weighting
```

---

#### 7. Design Twitter

See detailed walkthrough in Step 4 (Deep Dive section above)

**Additional Components:**

**Search (Elasticsearch):**
```
Tweets ‚Üí Kafka ‚Üí Elasticsearch indexer

Query:
GET /tweets/_search
{
  "query": {
    "multi_match": {
      "query": "system design",
      "fields": ["text", "hashtags"]
    }
  },
  "filter": {
    "range": {
      "created_at": {"gte": "now-7d"}
    }
  }
}
```

**Trending Topics:**
```python
# Count hashtags in real-time
from collections import Counter

class TrendingCalculator:
    def __init__(self):
        self.window = deque(maxlen=10000)  # Last 10K tweets
        
    def add_tweet(self, hashtags):
        self.window.append(hashtags)
        
    def get_trending(self, top_n=10):
        all_hashtags = [tag for tags in self.window for tag in tags]
        return Counter(all_hashtags).most_common(top_n)
```

---

#### 9. Design WhatsApp/Messenger

**Requirements:**
- 1-on-1 chat
- Group chat (up to 256 members)
- Online status
- Message delivery confirmations
- End-to-end encryption

**Architecture:**
```
[Mobile Client]
      ‚Üì WebSocket
[WebSocket Gateway]
      ‚Üì
[Chat Service]
  ‚Üì           ‚Üì
[Message Queue] [User Service]
  (Kafka)
  ‚Üì
[Message Store]
(Cassandra)
```

**Key Design Decisions:**

**1. WebSocket for Real-Time:**
```python
# Persistent connection
client.connect("wss://chat.whatsapp.com")

# Send message
client.send({
    "type": "message",
    "to": "user_123",
    "text": "Hello",
    "timestamp": 1642012345
})

# Receive message
client.on_message(lambda msg: display(msg))
```

**2. Message Queue:**
```
Sender ‚Üí WebSocket Gateway ‚Üí Kafka ‚Üí Receiver's Gateway ‚Üí Receiver

Benefits:
- Decouples sender and receiver
- Handles offline users (messages queued)
- Reliable delivery
```

**3. Database Schema:**
```sql
-- Messages (Cassandra)
CREATE TABLE messages (
    conversation_id UUID,
    message_id UUID,
    sender_id BIGINT,
    text TEXT,
    timestamp TIMESTAMP,
    delivery_status VARCHAR(20),
    PRIMARY KEY (conversation_id, timestamp, message_id)
) WITH CLUSTERING ORDER BY (timestamp DESC);

-- Online status (Redis)
Key: "online:{user_id}"
Value: "1" (online) or "0" (offline)
TTL: 30 seconds (refresh with heartbeat)
```

**4. Read Receipts:**
```
Message States:
1. Sent (one checkmark)
2. Delivered (two checkmarks)
3. Read (blue checkmarks)

Sender ‚Üí Message sent
       ‚Üì
Receiver's device receives ‚Üí Update to "delivered"
       ‚Üì
Receiver reads ‚Üí Update to "read"
       ‚Üì
Notify sender via WebSocket
```

**5. Group Chat:**
```
Approach: Fan-out to all members

User A posts to group (100 members):
  1. Store message once in database
  2. Create 100 delivery tasks in queue
  3. Each task sends message to one member
  
Optimization:
  - Don't wait for all deliveries
  - Return success to sender immediately
  - Deliver asynchronously
```

---

#### 10. Design Uber/Lyft

**Requirements:**
- Match riders with nearby drivers
- Real-time location tracking
- ETA calculation
- Dynamic pricing (surge)
- Trip history

**Architecture:**
```
[Driver App] ‚Üê‚Üí [WebSocket] ‚Üê‚Üí [Location Service] ‚Üê‚Üí [Redis (Geohash)]
[Rider App] ‚Üí [Matching Service] ‚Üí [QuadTree/Geohash] ‚Üí Find nearby drivers
            ‚Üí [Trip Service] ‚Üí [PostgreSQL]
            ‚Üí [Pricing Service]
            ‚Üí [Payment Service]
```

**Key Components:**

**1. Geospatial Indexing (Geohash):**
```python
# Geohash groups nearby locations
import geohash

# Encode lat/lng to geohash
loc = geohash.encode(37.7749, -122.4194, precision=6)
# Returns: "9q8yy"

# Find nearby drivers:
def find_nearby_drivers(rider_lat, rider_lng, radius_km):
    rider_geohash = geohash.encode(rider_lat, rider_lng, precision=6)
    
    # Get all drivers in same geohash + neighbors
    geohashes_to_search = [rider_geohash] + geohash.neighbors(rider_geohash)
    
    drivers = []
    for gh in geohashes_to_search:
        drivers.extend(redis.smembers(f"drivers:{gh}"))
    
    # Filter by exact distance
    return [d for d in drivers if distance(d, rider) <= radius_km]

# Update driver location
def update_driver_location(driver_id, lat, lng):
    new_geohash = geohash.encode(lat, lng, precision=6)
    
    # Remove from old geohash
    old_geohash = get_driver_geohash(driver_id)
    redis.srem(f"drivers:{old_geohash}", driver_id)
    
    # Add to new geohash
    redis.sadd(f"drivers:{new_geohash}", driver_id)
    
    # Update exact location
    redis.hset(f"driver:{driver_id}", "lat", lat, "lng", lng)
```

**2. Ride Matching Algorithm:**
```python
def match_rider_with_driver(ride_request):
    rider_location = ride_request.pickup_location
    
    # 1. Find nearby available drivers
    drivers = find_nearby_drivers(
        rider_location.lat,
        rider_location.lng,
        radius_km=5
    )
    
    # 2. Filter by car type
    drivers = [d for d in drivers if d.car_type == ride_request.car_type]
    
    # 3. Sort by proximity + rating
    drivers.sort(key=lambda d: (
        distance(d.location, rider_location) * 0.7 +
        (5 - d.rating) * 0.3
    ))
    
    # 4. Send request to closest driver
    for driver in drivers[:3]:  # Try top 3
        response = send_ride_request(driver, ride_request, timeout=15)
        if response.accepted:
            return create_trip(rider, driver, ride_request)
    
    return "No drivers available"
```

**3. ETA Calculation:**
```python
# Use Google Maps API or custom routing
def calculate_eta(driver_location, pickup_location):
    # Consider:
    # - Distance
    # - Current traffic
    # - Historical data (time of day)
    # - Road closures
    
    route = get_route(driver_location, pickup_location)
    base_time = route.distance / average_speed
    traffic_multiplier = get_traffic_multiplier(current_time, route)
    
    return base_time * traffic_multiplier
```

**4. Surge Pricing:**
```python
def calculate_price(pickup, dropoff, current_time):
    base_fare = 2.50
    per_km = 1.50
    per_minute = 0.30
    
    distance_km = calculate_distance(pickup, dropoff)
    estimated_duration = calculate_eta(pickup, dropoff)
    
    base_price = (
        base_fare +
        distance_km * per_km +
        estimated_duration * per_minute
    )
    
    # Surge multiplier based on supply/demand
    demand = get_active_ride_requests(pickup.geohash)
    supply = get_available_drivers(pickup.geohash)
    
    if demand > supply * 1.5:
        surge_multiplier = min(demand / supply, 3.0)
    else:
        surge_multiplier = 1.0
    
    return base_price * surge_multiplier
```

---

### Hard Problems (10)

#### 21. Design Google Search

**Requirements:**
- Web crawling (billions of pages)
- Indexing
- Query processing
- Ranking (relevance)
- Auto-complete
- Spell correction

**Architecture:**
```
Crawling:
[Seed URLs] ‚Üí [URL Frontier] ‚Üí [Crawler Workers] ‚Üí [HTML Parser]
                                      ‚Üì
                                [Content Store]
                                      ‚Üì
                                [Indexer]
                                      ‚Üì
                            [Inverted Index]

Query Processing:
[User Query] ‚Üí [Query Processor] ‚Üí [Search Index] ‚Üí [Ranker] ‚Üí [Results]
                                                        ‚Üì
                                                  [PageRank]
```

**Key Components:**

**1. Web Crawler:**
```python
class Crawler:
    def __init__(self):
        self.frontier = PriorityQueue()  # URLs to crawl
        self.seen = BloomFilter()        # Avoid duplicates
        self.robots_cache = {}           # robots.txt cache
        
    def crawl(self, seed_urls):
        for url in seed_urls:
            self.frontier.put((priority, url))
        
        while not self.frontier.empty():
            priority, url = self.frontier.get()
            
            if url in self.seen:
                continue
            
            if not self.can_crawl(url):
                continue
            
            html = self.fetch(url)
            links = self.extract_links(html)
            content = self.extract_content(html)
            
            # Store content
            self.store(url, content)
            
            # Add new links to frontier
            for link in links:
                if link not in self.seen:
                    self.frontier.put((calculate_priority(link), link))
            
            self.seen.add(url)
            self.politeness_delay()  # Don't overload servers
```

**2. Inverted Index:**
```
Document 1: "the quick brown fox"
Document 2: "the lazy dog"
Document 3: "quick brown dog"

Inverted Index:
"the"    ‚Üí [1, 2]
"quick"  ‚Üí [1, 3]
"brown"  ‚Üí [1, 3]
"fox"    ‚Üí [1]
"lazy"   ‚Üí [2]
"dog"    ‚Üí [2, 3]

Query "quick dog":
  "quick" ‚Üí [1, 3]
  "dog"   ‚Üí [2, 3]
  Intersection ‚Üí [3]  # Document 3 matches both
```

**3. PageRank Algorithm:**
```python
# Simplified PageRank
def calculate_pagerank(web_graph, iterations=20, d=0.85):
    N = len(web_graph.nodes)
    pagerank = {node: 1/N for node in web_graph.nodes}
    
    for _ in range(iterations):
        new_pagerank = {}
        for node in web_graph.nodes:
            rank_sum = sum(
                pagerank[linking_node] / len(web_graph.outlinks(linking_node))
                for linking_node in web_graph.inlinks(node)
            )
            new_pagerank[node] = (1 - d) / N + d * rank_sum
        pagerank = new_pagerank
    
    return pagerank
```

**4. Query Processing:**
```
User types: "system design interview"

Steps:
1. Tokenization: ["system", "design", "interview"]
2. Normalization: lowercase, remove stop words
3. Query expansion: Add synonyms ["architecture", "structure"]
4. Retrieve documents from inverted index
5. Rank by relevance (TF-IDF, PageRank, user signals)
6. Return top 10 results
```

**5. Auto-complete (Trie):**
```python
class TrieNode:
    def __init__(self):
        self.children = {}
        self.is_end = False
        self.frequency = 0

class AutoComplete:
    def __init__(self):
        self.root = TrieNode()
    
    def insert(self, word, frequency):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.is_end = True
        node.frequency = frequency
    
    def suggest(self, prefix, top_n=10):
        node = self.root
        for char in prefix:
            if char not in node.children:
                return []
            node = node.children[char]
        
        # DFS to find all words with this prefix
        suggestions = []
        self._dfs(node, prefix, suggestions)
        
        # Sort by frequency, return top N
        suggestions.sort(key=lambda x: x[1], reverse=True)
        return [word for word, freq in suggestions[:top_n]]
```

---

## üéØ PART 6: Interview Tips & Best Practices

### Communication Strategies

**Think Out Loud:**
```
‚úÖ Good: "I'm thinking we need a cache here because reads are 100x more than 
         writes, and latency is critical. Redis would work well for this."

‚ùå Bad: *silently draws diagram* "Here's my design."
```

**Ask Clarifying Questions:**
```
‚úÖ "When you say 'real-time', do you mean < 100ms or is 1-2 seconds acceptable?"
‚úÖ "Should we optimize for read or write performance?"
‚úÖ "Is strong consistency required or is eventual consistency okay?"

‚ùå "I'll just assume X and proceed."
```

**Discuss Trade-offs:**
```
‚úÖ "We could use SQL for ACID guarantees, but NoSQL would scale better.
     Given our read-heavy workload and need to handle 1M QPS, I'd recommend
     Cassandra with eventual consistency."

‚ùå "Let's use MongoDB." (No justification)
```

###Time Management

```
‚è∞ 0-7 min: Requirements (don't rush this!)
‚è∞ 7-12 min: Capacity estimation (quick, rough numbers)
‚è∞ 12-27 min: High-level design + deep dive (bulk of time)
‚è∞ 27-40 min: Deep dive specific components
‚è∞ 40-45 min: Wrap up, discuss improvements

Tips:
- If interviewer wants to deep dive early, follow their lead
- Don't get stuck on one component too long
- It's okay to say "We can dive deeper into X if you'd like"
```

### Red Flags vs Green Flags

**üö© Red Flags:**
- Jumping to solution without clarifying requirements
- Silent coding/designing
- Over-engineering simple problems
- Ignoring scalability ("We'll handle that later")
- Not discussing trade-offs
- Can't estimate numbers
- Single technology for everything

**‚úÖ Green Flags:**
- Asks clarifying questions
- Discusses multiple approaches
- Provides rough estimates
- Draws clear diagrams
- Explains reasoning
- Mentions monitoring/alerts
- Discusses failure scenarios
- Considers future extensibility

---

## üìä PART 7: Quick Reference Guide

### Technology Selection Matrix

| Need | Technology | Why |
|------|------------|-----|
| **ACID transactions** | PostgreSQL, MySQL | Strong consistency, complex queries |
| **High write throughput** | Cassandra, HBase | Distributed, column-family |
| **Simple key-value** | Redis, DynamoDB | Fast, in-memory or low latency |
| **Complex relationships** | Neo4j | Graph database, optimized for traversals |
| **Full-text search** | Elasticsearch | Inverted index, fast text queries |
| **Message queue** | Kafka, RabbitMQ | Async processing, decoupling |
| **Caching** | Redis, Memcached | Sub-millisecond latency |
| **Object storage** | S3, GCS | Large files, images, videos |
| **CDN** | CloudFlare, CloudFront | Global content delivery |

### Caching Strategy Selector

```
Choose caching strategy:

Read >> Write (100:1)?
  ‚Üí Cache-Aside (lazy loading)

Consistency critical?
  ‚Üí Write-Through (sync cache + DB)

High write performance needed?
  ‚Üí Write-Back (async DB writes)

Predictable access patterns?
  ‚Üí Refresh-Ahead (proactive refresh)
```

### Database Sharding Decision

```
When to shard:
‚úì Data > single DB capacity (>1TB)
‚úì Write throughput exceeds single DB
‚úì Need to scale horizontally

Sharding strategy:
Range-based:   Use when queries need ranges (dates, IDs)
Hash-based:    Use for even distribution
Geographic:    Use for global apps (data locality)
```

### Estimation Formulas

```python
# QPS
QPS = (DAU * actions_per_user_per_day) / 86400
Peak_QPS = QPS * 3

# Storage
storage_per_day = writes_per_day * size_per_write
annual_storage = storage_per_day * 365
with_replication = annual_storage * replication_factor

# Bandwidth
read_bandwidth = read_QPS * avg_response_size
write_bandwidth = write_QPS * avg_request_size

# Cache size (80-20 rule)
cache_size = total_storage * 0.2  # Cache hot 20% of data
```

---

## üìö PART 8: Study Resources & Plan

### 6-Week Study Plan

**Week 1-2: Fundamentals**
- [ ] Scalability concepts
- [ ] CAP theorem
- [ ] Load balancing
- [ ] Caching strategies
- [ ] Database types

**Week 3: Building Blocks**
- [ ] Message queues (Kafka)
- [ ] CDN
- [ ] API Gateway
- [ ] Rate limiting
- [ ] Circuit breaker

**Week 4: Patterns**
- [ ] Microservices
- [ ] Event-driven
- [ ] CQRS
- [ ] Saga pattern
- [ ] Practice: Design 5 easy problems

**Week 5: Practice Medium Problems**
- [ ] Design Twitter
- [ ] Design YouTube
- [ ] Design Uber
- [ ] Design WhatsApp
- [ ] Design Instagram

**Week 6: Practice Hard Problems + Mock Interviews**
- [ ] Design Google Search
- [ ] Design distributed cache
- [ ] Mock interviews (2-3)
- [ ] Review all concepts

### Essential Books

1. **"Designing Data-Intensive Applications"** by Martin Kleppmann
   - The bible of distributed systems
   - Deep dive into databases, distributed computing

2. **"System Design Interview Vol 1 & 2"** by Alex Xu
   - Great for interview prep
   - Visual diagrams, step-by-step walkthroughs

3. **"Web Scalability for Startup Engineers"** by Artur Ejsmont
   - Practical scaling strategies

### Online Resources

- **ByteByteGo** (YouTube) - Visual system design
- **Grokking the System Design Interview** (Educative.io)
- **SystemDesign.one** - Free diagrams and explanations
- **GitHub: system-design-primer** - Comprehensive guide
- **High Scalability Blog** - Real-world architectures

---

## ‚úÖ Final Interview Checklist

### Before Interview
- [ ] Review core concepts (CAP, scalability, caching)
- [ ] Practice 5-step framework
- [ ] Review 10 common problems
- [ ] Practice drawing diagrams
- [ ] Review estimation formulas

### During Interview
- [ ] Clarify ALL requirements (functional + non-functional)
- [ ] Do back-of-envelope calculations
- [ ] Draw high-level diagram first
- [ ] Explain your reasoning
- [ ] Discuss trade-offs
- [ ] Ask if interviewer wants to dive deeper
- [ ] Consider failure scenarios
- [ ] Mention monitoring/alerts

### Key Mantras
1. **"Let me clarify the requirements first"**
2. **"Let me do some quick capacity estimates"**
3. **"The trade-off here is..."**
4. **"We could also do X, but Y is better because..."**
5. **"For monitoring, we'd track..."**

---

## üéì Conclusion

You now have a comprehensive understanding of:

‚úÖ **Core Principles:**
- Scalability (vertical, horizontal, auto-scaling)
- CAP theorem with real examples
- Latency vs Throughput trade-offs

‚úÖ **Building Blocks:**
- Load Balancer (5 algorithms)
- Caching (4 strategies, 4 eviction policies)
- Databases (5 types, selection framework)
- Message Queues (Kafka deep dive)
- CDN, API Gateway

‚úÖ **Patterns:**
- Microservices
- Event-Driven Architecture
- CQRS, Saga
- Circuit Breaker

‚úÖ **Problem-Solving:**
- 5-step interview framework
- Capacity estimation formulas
- 30 complete problem solutions

‚úÖ **Interview Skills:**
- Communication strategies
- Time management
- Trade-off discussions
- Red flags vs green flags

### Next Steps

1. **Practice Daily:** Design 1 system every day
2. **Mock Interviews:** Practice with peers
3. **Read Real Architectures:** Study Netflix, Uber tech blogs
4. **Build Something:** Implement a small distributed system

### Remember

> **"Start simple, then scale"** - Don't over-engineer from the start

> **"There's no perfect solution"** - Always discuss trade-offs

> **"Communication > Knowledge"** - Explain your thinking clearly

---

**Good luck with your HLD interviews! üöÄ**

*Master the fundamentals, practice the framework, and approach each problem systematically. You've got this!*
